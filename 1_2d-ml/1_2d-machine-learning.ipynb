{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modul-Importe\n",
    "\n",
    "Zunächst importieren wir einige Module / Pakete, welche im Laufe dieses Notebooks genutzt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio\n",
    "import numpy as np # eine effiziente und weit verbreitete numerik-bibliothek. Note here: \"numpy\" haben wir als \"np\" eingeführt damit wir weniger tippe müssen. np ist ein alias für numpy.\n",
    "na = np.newaxis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import modules  # ein paket mit neural-network bausteinen. siehe modules/ und inhalt\n",
    "import model_io # ein modul zum speichern und laden von neuronalen netzwerken, die über modules definiert wurden.\n",
    "from datasets import generate_dataset, data_eval # liefert funktionen zum generieren von beispieldatensätzen. siehe datasets.py\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datensätze erkunden\n",
    "\n",
    "Das allererste, was man als Machine Learner tun sollte, bevor man die Problemösung angeht, ist die Daten zu untersuchen um diese zu verstehen. Nur so kann man mit dem durch die Inspektion erlangten Vorwissen bereits im Vorfeld einige Modelle und Methoden auswählen oder disqualifizieren. Wir haben im obigen code block die funktion `generate_dataset` aus dem modul `datasets.py` importiert, welche uns Zugriff zu einigen Methoden zum Erstellen von Beispieldatensätzen gewährt. Verschafft euch einen Überblick über die Signatur dieser methode und generiert einige Datensätze. \n",
    "\n",
    "Tipp: mit ein bis zwei Fragezeichen vor einem Funktionsnamen, zb `??generate_dataset`, kann man sich in JupyterLab die signatur, bzw den code der Methode ausgeben lassen.\n",
    "\n",
    "generate_dataset gibt drei Werte zurück. Der Erste ist ein numpy array (ie eine Matrix, nennen wir sie mal `X`, oder technisch betrachtet eher verschachtelte Vektoren) welches sich über die funktion `print(X)` ausgeben lassen. `X.shape` und `X.size` liefern euch informationen über die Form Dimensionalität der Arrays. Unter der Annahme, dass wir es hier mit 2d-Daten zu tun haben, was könnt ihr über die Daten herausfinden?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach einer sehr rudimentären Analyse der Daten wollen wir uns an deren Visualisierung wagen. Es macht immer Sinn, sich die (hier räumliche) Verteilung der Daten genauer anzusehen. Nutzt dazu die funktionen `plt.scatter` der Matplotlib-Bibliothek zum Zeichnen von Graphen und Daten(wolken). Nutzt die online verfügbare Dokumentation (oder fragt google mit menschenverständlichen Sätzen nach dem, was ihr erledigen wollt), um genauer zu spezifizieren, was ihr erledigen möchtet. Interessante stichworte, um auf gewisse Teile der Daten-Arrays zugreifen oder adressieren zu können sind \"subscripting\" oder \"slicing\". Weiterhin ist die Funktion `numpy.argmax` spannend für die Einfärbung der Datenpunkte nach Labelinformation. Sollte google, bzw die online-Dokumentation dieser Funktionien nicht helfen können, wendet euch an Sebastian.\n",
    "\n",
    "Sobald ihr eine rudimentäre Visualisierung umgesetzt habt, erkundet die verschiedenen Datensätze, welche über `generate_dataset` zur Verfügung gestellt werden, und welchen einfluss die verschiedenen Parameter auf die Funktion haben.\n",
    "\n",
    "Always remember: In ML and Data Science, visualization and understanding is key! We CAN run brute force e-to-end solutions, but usually efficiency will suffer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronale Netze (und andere Modelle) trainieren\n",
    "\n",
    "das Modul `modules` bietet eine Reihe von Bausteinen von Neuronalen Netzen (NNs). Um code block unten habe ich beispielhaft eine funktion geschrieben, welche ein NN-Architektur aufgebaut. Der Aufbau ist Typischerweise ein `Sequential` (ein Block, welcher eine Sequenz von NN-Blöcken enthält die der Reihe nach ausgeführt werden), welcher eine Liste von linearen oder nichtlinearen Funktionsblöcken enthält. Ändert die Funktion nach belieben ab, erweitert sie, etc, um Laufe dieses notebooks.\n",
    "\n",
    "Alternativ könnt ihr auch auf Modelle und Methoden aus der umfangreichen `sklearn`-Bibliothek zugreifen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(n_classes):\n",
    "    ##\n",
    "    ## ANPASSBARE Netzarchitektur:\n",
    "    ##\n",
    "    #build a network. add/remove layers. change number of nodes. observe effect on training\n",
    "    # dreifache anführungsstriche beginnen/beenden \"block-kommentare\"\n",
    "    # modules.Linear(n_in, n_out), modules.Tanh, modules.Rect, modules.SoftMax\n",
    "    \n",
    "    # Feel free to extend this function to support more Network architectures or ML Methods from sklearn\n",
    "    \n",
    "    \"\"\"\n",
    "    nn = modules.Sequential(\n",
    "        [modules.Linear(2,30),\n",
    "         modules.Tanh(),\n",
    "         modules.Linear(30,150),\n",
    "         modules.Tanh(),\n",
    "         modules.Linear(150,150),\n",
    "         modules.Tanh(),\n",
    "         modules.Linear(150,n_classes),\n",
    "         modules.SoftMax()\n",
    "        ]\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # ein lineares \"netz\" für den anfang\n",
    "    nn = modules.Sequential([modules.Linear(2,n_classes)])\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentales NN-Training\n",
    "\n",
    "Um NNs zu trainieren verwendet man in der Regel einen Algorithmus namens Stochastic Gradient Descent (SGD). Stochastisch bedeutet, dass hier immer anhand von wenigen aus dem Datensatz gezogener Beispiele Trainiert wird (ähnlich wie wir menschen lernen -- anhand einzelner Beispiele). Der Gradient Descent - Teil von SGD beschreibt die Optimierungsmethodik: Mit jeder während des Trainings ausgeführten Vorhersage wird ein Fehlersignal berechnet, welches als Gradient durch das Netzwerk zurückgeschickt wird und beschreibt, in welche Richtung die Parameter (ie die gelernten Gewichte) angepasst werden müssen um den Fehler zu minimieren.\n",
    "\n",
    "Man kann sich das wie folgt vorstellen: Die Parameter des Netzes beschreiben einen Anzahl(Parameter)-Dimensionalen Raum, in dem sich das Modell (zb während der Optimierung) bewegen kann, während es eine Mathematische Funktion beschreibt, die von den Parametern abhängig ist. Im stochastischen lernprozess werden nun stichpunkthaft Fehlersignale dieser Funktion errechnet, welche ableitbar ist. Das Ziel ist es, den Fehler zu minimieren, dh die \"Fehlerlandschaft\" bergab in das niedrigste Tal zu rutschen. Diese bergab-Richtung gibt der Gradient vor.\n",
    "\n",
    "In jedem Update-Schritt des Netzes  korrigieren wir also die Parameter mit zufällig gezogenen Daten ein kleines bisschen, bis wir so nahe am Optimum wie nur möglich angekommen sind. Der stochastische Lernprozess erlaubt es uns hier, mit beliebig großen Datenmengen und (fast) beliebig großen Modellparameterräumen umzugehen, weil wir immer eine Auswertung nach der anderen betrachten und mit baby steps in die (hoffentlich) richtige Richtung wandern.\n",
    "\n",
    "Andere methoden, wie beispielsweise Support Vector Machines (SVMs) optimieren die Parameter mit der Lösug von Gleichungssystemen in geschlossener Form, was ein gleichzeitiges betrachten aller Parameter und Datenpunkte (in interaktion) erfordert, was oft rechnerisch bei gewissen Problemen an praktische und theoretische Grenzen stößt (jedoch vermutlich nicht hier in dieser 2d-Sandbox).\n",
    "\n",
    "TL;DR: im Block unter diesem findet ihr den rudimentären Aufbau eines NN-Trainings-Algorithmus. Spielt mit der Modellarchitektur oben (gerne so extrem ihr wollt. Fangt mit einem einzelnen Linear(num_dims, num_classes) an!), sowie den Trainigsparametern unten, und beobachtet, wie gut sich das Netz auf die Trainingsdaten anpassen kann, indem ihr den Loss (ie den Prediktionsfehler) beobachtet.\n",
    "\n",
    "Wichtig: Damit aus dem ganzen Prozess Wissenschaft wird, dokumentiert bitte eure Beobachtungen, und Hypothesen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Epoch 000: Loss 0.590476, Acc 96.40%\n",
      "After Epoch 001: Loss 0.476591, Acc 99.90%\n",
      "After Epoch 002: Loss 0.368358, Acc 100.00%\n",
      "After Epoch 003: Loss 0.279637, Acc 100.00%\n",
      "After Epoch 004: Loss 0.209307, Acc 100.00%\n",
      "After Epoch 005: Loss 0.161147, Acc 100.00%\n",
      "After Epoch 006: Loss 0.125414, Acc 100.00%\n",
      "After Epoch 007: Loss 0.100910, Acc 100.00%\n",
      "After Epoch 008: Loss 0.082408, Acc 100.00%\n",
      "After Epoch 009: Loss 0.068610, Acc 100.00%\n",
      "After Epoch 010: Loss 0.058513, Acc 100.00%\n",
      "After Epoch 011: Loss 0.050552, Acc 100.00%\n",
      "After Epoch 012: Loss 0.044214, Acc 100.00%\n",
      "After Epoch 013: Loss 0.039035, Acc 100.00%\n",
      "After Epoch 014: Loss 0.034772, Acc 100.00%\n",
      "After Epoch 015: Loss 0.031206, Acc 100.00%\n",
      "After Epoch 016: Loss 0.028265, Acc 100.00%\n",
      "After Epoch 017: Loss 0.025801, Acc 100.00%\n",
      "After Epoch 018: Loss 0.023632, Acc 100.00%\n",
      "After Epoch 019: Loss 0.021725, Acc 100.00%\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## ANPASSBARE PARAMETER:\n",
    "##\n",
    "datensatz = 'xor' # 'three',  'four', 'xor', 'circles', 'moons' # Verfügbare Datensätze\n",
    "n_data = 1000  # Anzahl Datenpunkte im Trainingsdatensatz\n",
    "seed = 1234# random_seed\n",
    "\n",
    "epochs = 20   # anzahl trainings-epochen, dh wie oft wiederholen wir den datensatz im training?\n",
    "batchsize = 10  # batch size pro trainings-ITERATION. MUSS KLEINER SEIN ALS ANZAHL TRAININGSDATEN. wieviele daten werden gleichzeitig genutzt um ein update des models zu berechnen?\n",
    "lrate = 0.001     # lernrate / schrittgröße der korrekturen beim training\n",
    "iterations_per_epoch = n_data//batchsize  # eine kanonische epoche = einmal durchs dataset. wir passen hier daher die #iterations an anhand von batchsize und dataset size an. // ist integer division. / wäre float division\n",
    "\n",
    "\n",
    "\n",
    "##\n",
    "## AB HIER: Trainings/Evaluierungs-Pipeline\n",
    "##\n",
    "\n",
    "# generiere Trainingsdaten und Evaluierungsdaten\n",
    "X,Y,n_classes = generate_dataset(name=datensatz, N=n_data, random_seed=seed)\n",
    "\n",
    "# generiere neues Netzwerk:\n",
    "nn = build_network(n_classes)\n",
    "\n",
    "# Trainiere ein Netzwerk für mehrere Epochen,\n",
    "# mit der gleichen Anzahl an iterationen pro Epoche.\n",
    "# Zwischen den Epochen wird das Netz ausgewertet\n",
    "\n",
    "train_performance = [] #record the loss over epochs for the whole training set.\n",
    "for e in range(epochs):\n",
    "    # train the network for one epoch. Have a looka the training code in modules.sequential.Sequential.train and other modules to see how it works!\n",
    "    nn.train(X,Y, batchsize=batchsize, lrate=lrate, iters=iterations_per_epoch, status=iterations_per_epoch, silent=True) # set silent=True if you want stuff on the command line    \n",
    "    \n",
    "    #predict on whole training set\n",
    "    y_pred = nn.forward(X)\n",
    "    \n",
    "    # compute loss and classification accuracy\n",
    "    l2_loss = np.mean(np.sqrt(np.sum((y_pred - Y)**2, axis=1)))  #try out alternative loss computations. this is for now the euclidean distance between predicted output and expected output per sample, averaged over all samples\n",
    "    acc = np.mean(np.argmax(y_pred, axis=1) == np.argmax(Y, axis=1)) # accuracy = average (ie mean) \"hit\" probability of predicting the correct class\n",
    "    \n",
    "    print('After Epoch {:03d}: Loss {:.6f}, Acc {:.2f}%'.format(e, l2_loss, acc*100))\n",
    "    \n",
    "    train_performance.append((l2_loss, acc)) # add tuples of (loss, accuracy) to the epoch  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Visualisierung der Trainingsverlust- und Genauigkeitskurven in train_performance\n",
    "\n",
    "Verwendet nun `plt.plot`, um x-Koordinaten (Epochenindizes), y-Koordinaten (Verlust oder Genauigkeit) und den Parameter `label` zu setzen, um zu visualisieren, wie sich die Trainingsleistung über die Epochen verändert!\n",
    "Ihr könnt ihre Darstellung lesbarer machen, indem ihr z.B. `plt.xlabel()`, `plt.ylabel()`, `plt.legend()` und/oder `plt.title()` verwendet.\n",
    "Was seht ihr (in verschiedenen Einstellungen)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn die Leistung eines Vorhersagemodells für maschinelles Lernen bewertet wird, ist es in der Regel wenig sinnvoll, dies auf den Trainingsdaten zu tun, da wir dann nicht wissen, ob das Modell die Lösung für die bereitgestellten Datenpunkte \"auswendig\" gelernt hat oder ob es wirklich verallgemeinerbare Regeln gefunden hat, die auch auf unbekannte Datenpunkte anwendbar sind. Daher ist es gängige Praxis, die endgültige (aber auch zwischenzeitliche) Leistung des Modells an zuvor nicht gesehenen Testdaten zu evaluieren. Zu diesem Zweck erzeugt einen neuen Datensatz (mit derselben Verteilung/demselben Typ) wie zuvor und testet euer Modell damit. Vergleicht Sie die (endgültige) Testleistung des Modells im gegebenen Zustand mit der (endgültigen) Trainingsleistung. Es könnte sinnvoll sein, dies mit einem Modell mit besserer Leistung (als nur einem linearen Klassifizierer) zu evaluieren.\n",
    "\n",
    "Sobald ihr dies erfolgreich getan habt, fügt den Code für die wiederholte Schätzung der Testleistung in die vorherige Trainingsschleife ein.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisierung der erlernten Entscheidungslinie\n",
    "\n",
    "Um weitere Einblicke zu erhalten, wie sich unser trainiertes Modell auf allen möglichen unbekannten Daten verhält -- nicht nur auf dem extra generierte Testdatensatz -- gibt es die funktion `data_eval`, mit der wir den Inputspace dicht abstecken und auswerten können.\n",
    "\n",
    "Macht euch mit der Funktion vertraut und generiert einen Evaluierungsdatensatz, den ihr mit dem Modell auswertet.\n",
    "Nutzt anschließend die Hilfsfunktionen in diesem codeblock, um die Entscheidungslinie des Modells zu visualisieren.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Some analysis helper functions\n",
    "\n",
    "def analyze_decision_boundary(neuralnet, Xtrain, Ytrain, Xeval, gridcoords, canvas_buffer):\n",
    "    # predict evaluation data with neural net\n",
    "    Yeval = neuralnet.forward(Xeval)\n",
    "    \n",
    "    predictions = np.argmax(Yeval, axis=1)\n",
    "    levels = np.arange(Yeval.shape[1])\n",
    "    XX, YY = gridcoords\n",
    "    # bring predictions to grid shape\n",
    "    predictions = np.argmax(Yeval, axis=1)\n",
    "    predictions = np.reshape(predictions, XX.shape)\n",
    "      \n",
    "\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    fig = plt.figure(plt.gcf())\n",
    "\n",
    "        \n",
    "    # render grid evaluation\n",
    "    plt.contourf(XX, YY, predictions, alpha=0.5)\n",
    "    plt.contour(XX, YY, predictions)\n",
    "    plt.scatter(Xtrain[:,0], Xtrain[:,1], c=np.argmax(Ytrain, axis=1))\n",
    "        \n",
    "    # plot data\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "    \n",
    "    # collect figure canvas for later processing \n",
    "    fig.canvas.draw()\n",
    "    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "    image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    canvas_buffer.append(image)\n",
    "    \n",
    "\n",
    "# can be used to save visualizations\n",
    "def save_visualizations(output_dir, canvas_buffer, fps=1.0):\n",
    "    print('Saving as animated gif to {} ...'.format(output_dir))\n",
    "    imageio.mimsave('{}/animation.gif'.format(output_dir), canvas_buffer, fps=fps)\n",
    "    print('Saving individual PNGs to {} ...'.format(output_dir))\n",
    "    output_dir += '/frames'\n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for i, frame in enumerate(canvas_buffer):\n",
    "        imageio.imwrite('{}/epoch-{}.png'.format(output_dir, i), frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baut nun die visualisierung der Entscheidungslinie in den Trainingsloop weiter oben ein und beobachtet, wie sich das Model (und somit die Entscheidungslinine) im Laufe des trainings auf die Daten anpasst!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonusaufgabe:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ihr wisst nun, wie man ML-Modelle auf diversen Datensätzen optimiert und evaluiert. Programmiert nun eine rigorose Auswertung einer Reihe von Modellen (durchforstet gern auch `sklearn`) über die euch zur verfügung stehenden Datensätze und und vergleich deren Performance.\n",
    "\n",
    "Legt zb. eine Liste von Modellen an, die ihr evaluieren wollt, und eine Liste von Datensätzen. Trainiert jedes Modell auf jedem Datensatz und messt die Performance auf den Trainings- aber auch auf den Testdaten.\n",
    "\n",
    "Visualisiert am ende die Ergebnisse, zb in einer Tabelle oder einem scatter plot, oder wie es euch am besten passt, um herauszufinden, welches Modell in welchem Setting zu bevorzugen wäre. Stellt Hypothesen auf und diskutiert diese, wieso ihr zu euren Schlüssen und Ergebnissen kommt.\n",
    "\n",
    "Um euch Zeit beim debugging und training zu sparen, nutzt die `read` und `write` funktionen im modul `model_io`, um NNs abzuspeichern und zu laden, vorzugsweise im plain text format (`fmt=\"txt\"`) um einen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
